# ğŸ“¨ Research-to-Email Assistant (Gemini + LangChain + Judgeval)

Generate clear, stakeholder-friendly email digests from any topic **in one CLI command** â€“ with full tracing and online evaluations powered by **Judgment Labs / judgeval**.

<br>

| Stack | Why I chose it |
|-------|-----------------|
| **Google Gemini 2.0 Flash** | Fast, cost-efficient model for concise summaries |
| **LangChain** | Easy tool/LLM chaining with a huge ecosystem |
| **judgeval** | Real-time tracing + quality metrics (Answer Relevancy) |

<br>

## âœ¨ Features
* ğŸ” **Web & Wikipedia search** (Serper Google Search optional)\
* ğŸ“ **Gemini-drafted email** capped at 180 words\
* ğŸ“ˆ **Judgeval tracing** â€“ every run auto-streams to the Judgment dashboard\
* âœ… **Answer Relevancy eval**  (online) â€“ flags off-topic emails\
* Zero OpenAI keys required

---

## ğŸš€ Quick Start

```bash
git clone <your-fork> && cd research_email_agent
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

cp .env.example .env          # fill in keys
# â””â”€ GEMINI_API_KEY=<your-key>
# â””â”€ (optional) SERPER_API_KEY=<google search key>
# â””â”€ JUDGMENT_API_KEY / JUDGMENT_ORG_ID  â† get free at app.judgmentlabs.ai

python run.py "Explain how AI works in simple terms"
```

Youâ€™ll see:
```bash
ğŸ” View Trace: judgeval trace link here
=== Draft Email ===
Subject: AI Explained Simply
...
```

## ğŸ—‚ Project Structure
```bash
research_email_agent/
â”œâ”€ agent/                       # core logic
â”‚   â”œâ”€ tools/                   # WebSearchTool + EmailDraftTool
â”‚   â”œâ”€ main_agent.py            # tracing + eval orchestration
â”‚   â””â”€ config.py
â”œâ”€ run.py                       # CLI entry
â”œâ”€ .env.example                 # sample secrets
â”œâ”€ requirements.txt
â””â”€ traces/                      # local JSON traces (git-ignored)
```

## ğŸ› ï¸ How Tracing & Evals Work

1. A global Tracer object is created in main_agent.py

2. Functions are decorated with @judgment.observe(span_type="â€¦") â†’ spans auto-record inputs/outputs.

3. After drafting the email, we call:
   ```bash
   judgment.async_evaluate(
    scorers=[AnswerRelevancyScorer(threshold=0.5)],
    input=research,
    actual_output=email,
    model="gpt-4o-mini",
   )
   ```

Judgeval queues the eval; the score appears on the `draft_email_step` span in seconds.

## ğŸ¤ Contributing
PRs, issues, and docs tweaks welcome!

